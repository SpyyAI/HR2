{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score, roc_curve, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import itertools\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "SEED = 123\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "random.seed(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Load dataset\n",
    "df = pd.read_csv('recruitment_data.csv')\n",
    "\n",
    "# Define the features and the target class\n",
    "x = df.drop(columns=['HiringDecision'], axis=1)\n",
    "y = df['HiringDecision']\n",
    "\n",
    "# Split the data\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=123)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Load the saved models\n",
    "model_paths = [\n",
    "    'best_svm_model.pkl',\n",
    "    'catBoost1model.pkl',\n",
    "    'best_knn_model.pkl',\n",
    "    'LR_model_imb.pkl',\n",
    "    'best_gnb_model.pkl',\n",
    "    'RFmodel.pkl',\n",
    "    'xgbimba.pkl',\n",
    "    'DTmodel89.pkl'\n",
    "]\n",
    "\n",
    "models = []\n",
    "for path in model_paths:\n",
    "    try:\n",
    "        model = joblib.load(path)\n",
    "        if hasattr(model, 'fit'):\n",
    "            models.append(model)\n",
    "        else:\n",
    "            print(f\"Error: Loaded object from {path} is not a valid model.\")\n",
    "    except ModuleNotFoundError as e:\n",
    "        print(f\"ModuleNotFoundError for {path}: {e}\")\n",
    "\n",
    "model_names = [f'model_{i+1}' for i in range(len(models))]\n",
    "estimators = list(zip(model_names, models))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Define possible voting schemes\n",
    "voting_schemes = ['hard', 'soft']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy with voting=hard and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))]))): 91.33%\n",
      "Test Accuracy with voting=soft and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))]))): 92.22%\n",
      "Failed with voting=hard and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=soft and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Test Accuracy with voting=hard and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): 90.67%\n",
      "Test Accuracy with voting=soft and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): 94.67%\n",
      "Test Accuracy with voting=hard and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 96.00%\n",
      "Test Accuracy with voting=soft and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 96.00%\n",
      "Test Accuracy with voting=hard and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 95.11%\n",
      "Test Accuracy with voting=soft and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 96.00%\n",
      "Test Accuracy with voting=hard and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 95.11%\n",
      "Test Accuracy with voting=soft and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 95.11%\n",
      "Failed with voting=hard and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=soft and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Test Accuracy with voting=hard and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): 89.11%\n",
      "Test Accuracy with voting=soft and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): 86.00%\n",
      "Test Accuracy with voting=hard and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 90.89%\n",
      "Test Accuracy with voting=soft and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 88.22%\n",
      "Test Accuracy with voting=hard and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 90.00%\n",
      "Test Accuracy with voting=soft and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 89.11%\n",
      "Test Accuracy with voting=hard and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 88.89%\n",
      "Test Accuracy with voting=soft and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 88.00%\n",
      "Failed with voting=hard and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=soft and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=hard and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=soft and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=hard and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=soft and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=hard and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=soft and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Test Accuracy with voting=hard and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 90.44%\n",
      "Test Accuracy with voting=soft and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 91.78%\n",
      "Test Accuracy with voting=hard and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 89.78%\n",
      "Test Accuracy with voting=soft and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 91.11%\n",
      "Test Accuracy with voting=hard and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 89.33%\n",
      "Test Accuracy with voting=soft and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 90.89%\n",
      "Test Accuracy with voting=hard and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 94.44%\n",
      "Test Accuracy with voting=soft and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 94.67%\n",
      "Test Accuracy with voting=hard and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 94.00%\n",
      "Test Accuracy with voting=soft and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 93.78%\n",
      "Test Accuracy with voting=hard and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 93.11%\n",
      "Test Accuracy with voting=soft and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 93.56%\n",
      "Failed with voting=hard and models (('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=soft and models (('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Test Accuracy with voting=hard and models (('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): 91.56%\n",
      "Test Accuracy with voting=soft and models (('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): 93.33%\n",
      "Test Accuracy with voting=hard and models (('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 96.00%\n",
      "Test Accuracy with voting=soft and models (('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 96.22%\n",
      "Test Accuracy with voting=hard and models (('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 94.67%\n",
      "Test Accuracy with voting=soft and models (('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 95.33%\n",
      "Test Accuracy with voting=hard and models (('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 93.11%\n",
      "Test Accuracy with voting=soft and models (('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 92.89%\n",
      "Failed with voting=hard and models (('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=soft and models (('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=hard and models (('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=soft and models (('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=hard and models (('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=soft and models (('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=hard and models (('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=soft and models (('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Test Accuracy with voting=hard and models (('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 95.78%\n",
      "Test Accuracy with voting=soft and models (('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 96.22%\n",
      "Test Accuracy with voting=hard and models (('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 95.11%\n",
      "Test Accuracy with voting=soft and models (('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 96.22%\n",
      "Test Accuracy with voting=hard and models (('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 95.11%\n",
      "Test Accuracy with voting=soft and models (('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 95.56%\n",
      "Test Accuracy with voting=hard and models (('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 95.78%\n",
      "Test Accuracy with voting=soft and models (('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 96.22%\n",
      "Test Accuracy with voting=hard and models (('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.22%\n",
      "Test Accuracy with voting=soft and models (('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.00%\n",
      "Test Accuracy with voting=hard and models (('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 95.33%\n",
      "Test Accuracy with voting=soft and models (('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.00%\n",
      "Failed with voting=hard and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=soft and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=hard and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=soft and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=hard and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=soft and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=hard and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=soft and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Test Accuracy with voting=hard and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 91.78%\n",
      "Test Accuracy with voting=soft and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 88.00%\n",
      "Test Accuracy with voting=hard and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 90.22%\n",
      "Test Accuracy with voting=soft and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 88.89%\n",
      "Test Accuracy with voting=hard and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 88.67%\n",
      "Test Accuracy with voting=soft and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 88.44%\n",
      "Test Accuracy with voting=hard and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 94.00%\n",
      "Test Accuracy with voting=soft and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 92.00%\n",
      "Test Accuracy with voting=hard and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 92.44%\n",
      "Test Accuracy with voting=soft and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 90.22%\n",
      "Test Accuracy with voting=hard and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 92.00%\n",
      "Test Accuracy with voting=soft and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 90.89%\n",
      "Failed with voting=hard and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=soft and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=hard and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=soft and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=hard and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=soft and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=hard and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=soft and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=hard and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=soft and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=hard and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=soft and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Test Accuracy with voting=hard and models (('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 94.22%\n",
      "Test Accuracy with voting=soft and models (('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 94.67%\n",
      "Test Accuracy with voting=hard and models (('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 94.67%\n",
      "Test Accuracy with voting=soft and models (('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 93.11%\n",
      "Test Accuracy with voting=hard and models (('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 93.11%\n",
      "Test Accuracy with voting=soft and models (('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 93.78%\n",
      "Test Accuracy with voting=hard and models (('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 94.89%\n",
      "Test Accuracy with voting=soft and models (('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 94.44%\n",
      "Failed with voting=hard and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=soft and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Test Accuracy with voting=hard and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): 90.67%\n",
      "Test Accuracy with voting=soft and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): 91.78%\n",
      "Test Accuracy with voting=hard and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 93.33%\n",
      "Test Accuracy with voting=soft and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 94.44%\n",
      "Test Accuracy with voting=hard and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 92.44%\n",
      "Test Accuracy with voting=soft and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 94.67%\n",
      "Test Accuracy with voting=hard and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 91.11%\n",
      "Test Accuracy with voting=soft and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 93.78%\n",
      "Failed with voting=hard and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=soft and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=hard and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=soft and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=hard and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=soft and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=hard and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=soft and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Test Accuracy with voting=hard and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 92.67%\n",
      "Test Accuracy with voting=soft and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 95.56%\n",
      "Test Accuracy with voting=hard and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 92.22%\n",
      "Test Accuracy with voting=soft and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 95.33%\n",
      "Test Accuracy with voting=hard and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 91.56%\n",
      "Test Accuracy with voting=soft and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 95.33%\n",
      "Test Accuracy with voting=hard and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 94.89%\n",
      "Test Accuracy with voting=soft and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 96.00%\n",
      "Test Accuracy with voting=hard and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.00%\n",
      "Test Accuracy with voting=soft and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 95.78%\n",
      "Test Accuracy with voting=hard and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 94.89%\n",
      "Test Accuracy with voting=soft and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 95.78%\n",
      "Failed with voting=hard and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=soft and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=hard and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=soft and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=hard and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=soft and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=hard and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=soft and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Test Accuracy with voting=hard and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 90.89%\n",
      "Test Accuracy with voting=soft and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 89.11%\n",
      "Test Accuracy with voting=hard and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 90.22%\n",
      "Test Accuracy with voting=soft and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 89.33%\n",
      "Test Accuracy with voting=hard and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 88.89%\n",
      "Test Accuracy with voting=soft and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 88.67%\n",
      "Test Accuracy with voting=hard and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 92.44%\n",
      "Test Accuracy with voting=soft and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 92.00%\n",
      "Test Accuracy with voting=hard and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 91.11%\n",
      "Test Accuracy with voting=soft and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 91.33%\n",
      "Test Accuracy with voting=hard and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 90.44%\n",
      "Test Accuracy with voting=soft and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 91.78%\n",
      "Failed with voting=hard and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=soft and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=hard and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=soft and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=hard and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=soft and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=hard and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=soft and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=hard and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=soft and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=hard and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=soft and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Test Accuracy with voting=hard and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 92.00%\n",
      "Test Accuracy with voting=soft and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 94.00%\n",
      "Test Accuracy with voting=hard and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 91.78%\n",
      "Test Accuracy with voting=soft and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 93.33%\n",
      "Test Accuracy with voting=hard and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 91.11%\n",
      "Test Accuracy with voting=soft and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 93.33%\n",
      "Test Accuracy with voting=hard and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 94.44%\n",
      "Test Accuracy with voting=soft and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 93.78%\n",
      "Failed with voting=hard and models (('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=soft and models (('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=hard and models (('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=soft and models (('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=hard and models (('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=soft and models (('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=hard and models (('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=soft and models (('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Test Accuracy with voting=hard and models (('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 92.89%\n",
      "Test Accuracy with voting=soft and models (('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 94.89%\n",
      "Test Accuracy with voting=hard and models (('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 92.22%\n",
      "Test Accuracy with voting=soft and models (('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 94.67%\n",
      "Test Accuracy with voting=hard and models (('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 91.56%\n",
      "Test Accuracy with voting=soft and models (('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 93.33%\n",
      "Test Accuracy with voting=hard and models (('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 95.33%\n",
      "Test Accuracy with voting=soft and models (('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 95.33%\n",
      "Test Accuracy with voting=hard and models (('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 95.11%\n",
      "Test Accuracy with voting=soft and models (('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 94.67%\n",
      "Test Accuracy with voting=hard and models (('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 93.78%\n",
      "Test Accuracy with voting=soft and models (('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 95.11%\n",
      "Failed with voting=hard and models (('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=soft and models (('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=hard and models (('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=soft and models (('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=hard and models (('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=soft and models (('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=hard and models (('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=soft and models (('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=hard and models (('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=soft and models (('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=hard and models (('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=soft and models (('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Test Accuracy with voting=hard and models (('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 94.89%\n",
      "Test Accuracy with voting=soft and models (('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 95.78%\n",
      "Test Accuracy with voting=hard and models (('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 95.33%\n",
      "Test Accuracy with voting=soft and models (('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 95.78%\n",
      "Test Accuracy with voting=hard and models (('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 94.44%\n",
      "Test Accuracy with voting=soft and models (('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 95.78%\n",
      "Test Accuracy with voting=hard and models (('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 95.78%\n",
      "Test Accuracy with voting=soft and models (('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.22%\n",
      "Failed with voting=hard and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=soft and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=hard and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=soft and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=hard and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=soft and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=hard and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=soft and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=hard and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=soft and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=hard and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=soft and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Test Accuracy with voting=hard and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 92.22%\n",
      "Test Accuracy with voting=soft and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 92.00%\n",
      "Test Accuracy with voting=hard and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 91.78%\n",
      "Test Accuracy with voting=soft and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 91.56%\n",
      "Test Accuracy with voting=hard and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 90.44%\n",
      "Test Accuracy with voting=soft and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 92.00%\n",
      "Test Accuracy with voting=hard and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 93.56%\n",
      "Test Accuracy with voting=soft and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 92.44%\n",
      "Failed with voting=hard and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=soft and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=hard and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=soft and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=hard and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=soft and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=hard and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=soft and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Test Accuracy with voting=hard and models (('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 94.00%\n",
      "Test Accuracy with voting=soft and models (('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 94.00%\n",
      "Failed with voting=hard and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=soft and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=hard and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=soft and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=hard and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=soft and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=hard and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=soft and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Test Accuracy with voting=hard and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 92.22%\n",
      "Test Accuracy with voting=soft and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 94.00%\n",
      "Test Accuracy with voting=hard and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 91.56%\n",
      "Test Accuracy with voting=soft and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 93.78%\n",
      "Test Accuracy with voting=hard and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 90.89%\n",
      "Test Accuracy with voting=soft and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 92.89%\n",
      "Test Accuracy with voting=hard and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 94.89%\n",
      "Test Accuracy with voting=soft and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 94.67%\n",
      "Test Accuracy with voting=hard and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 94.67%\n",
      "Test Accuracy with voting=soft and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 94.67%\n",
      "Test Accuracy with voting=hard and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 94.00%\n",
      "Test Accuracy with voting=soft and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 95.11%\n",
      "Failed with voting=hard and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=soft and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=hard and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=soft and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=hard and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=soft and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=hard and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=soft and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=hard and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=soft and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=hard and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=soft and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Test Accuracy with voting=hard and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 94.67%\n",
      "Test Accuracy with voting=soft and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 95.56%\n",
      "Test Accuracy with voting=hard and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 95.33%\n",
      "Test Accuracy with voting=soft and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.22%\n",
      "Test Accuracy with voting=hard and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 94.44%\n",
      "Test Accuracy with voting=soft and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 95.56%\n",
      "Test Accuracy with voting=hard and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 95.33%\n",
      "Test Accuracy with voting=soft and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 95.78%\n",
      "Failed with voting=hard and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=soft and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=hard and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=soft and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=hard and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=soft and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=hard and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=soft and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=hard and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=soft and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=hard and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=soft and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Test Accuracy with voting=hard and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 91.56%\n",
      "Test Accuracy with voting=soft and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 92.44%\n",
      "Test Accuracy with voting=hard and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 90.89%\n",
      "Test Accuracy with voting=soft and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 90.67%\n",
      "Test Accuracy with voting=hard and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 90.22%\n",
      "Test Accuracy with voting=soft and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 91.33%\n",
      "Test Accuracy with voting=hard and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 93.33%\n",
      "Test Accuracy with voting=soft and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 92.89%\n",
      "Failed with voting=hard and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=soft and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=hard and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=soft and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=hard and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=soft and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=hard and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=soft and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Test Accuracy with voting=hard and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 94.00%\n",
      "Test Accuracy with voting=soft and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 93.78%\n",
      "Failed with voting=hard and models (('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=soft and models (('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=hard and models (('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=soft and models (('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=hard and models (('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=soft and models (('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=hard and models (('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=soft and models (('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=hard and models (('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=soft and models (('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=hard and models (('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=soft and models (('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Test Accuracy with voting=hard and models (('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 95.11%\n",
      "Test Accuracy with voting=soft and models (('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 95.11%\n",
      "Test Accuracy with voting=hard and models (('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 95.33%\n",
      "Test Accuracy with voting=soft and models (('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 94.67%\n",
      "Test Accuracy with voting=hard and models (('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 94.22%\n",
      "Test Accuracy with voting=soft and models (('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 94.89%\n",
      "Test Accuracy with voting=hard and models (('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 95.11%\n",
      "Test Accuracy with voting=soft and models (('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 95.33%\n",
      "Failed with voting=hard and models (('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=soft and models (('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=hard and models (('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=soft and models (('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=hard and models (('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=soft and models (('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=hard and models (('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=soft and models (('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Test Accuracy with voting=hard and models (('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 95.56%\n",
      "Test Accuracy with voting=soft and models (('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.00%\n",
      "Failed with voting=hard and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=soft and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=hard and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=soft and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=hard and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=soft and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=hard and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=soft and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Test Accuracy with voting=hard and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 94.00%\n",
      "Test Accuracy with voting=soft and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 92.67%\n",
      "Failed with voting=hard and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=soft and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=hard and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=soft and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=hard and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=soft and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=hard and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=soft and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=hard and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=soft and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=hard and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=soft and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=hard and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=soft and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Test Accuracy with voting=hard and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 92.89%\n",
      "Test Accuracy with voting=soft and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 94.67%\n",
      "Test Accuracy with voting=hard and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 92.89%\n",
      "Test Accuracy with voting=soft and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 94.22%\n",
      "Test Accuracy with voting=hard and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 92.22%\n",
      "Test Accuracy with voting=soft and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 94.44%\n",
      "Test Accuracy with voting=hard and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 95.11%\n",
      "Test Accuracy with voting=soft and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 95.33%\n",
      "Failed with voting=hard and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=soft and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=hard and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=soft and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=hard and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=soft and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=hard and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=soft and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Test Accuracy with voting=hard and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 94.89%\n",
      "Test Accuracy with voting=soft and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 95.78%\n",
      "Failed with voting=hard and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=soft and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=hard and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=soft and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=hard and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=soft and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=hard and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=soft and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Test Accuracy with voting=hard and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 92.44%\n",
      "Test Accuracy with voting=soft and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 92.22%\n",
      "Failed with voting=hard and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=soft and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=hard and models (('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=soft and models (('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=hard and models (('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=soft and models (('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=hard and models (('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=soft and models (('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=hard and models (('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=soft and models (('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Test Accuracy with voting=hard and models (('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 94.89%\n",
      "Test Accuracy with voting=soft and models (('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 95.33%\n",
      "Failed with voting=hard and models (('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=soft and models (('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=hard and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=soft and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=hard and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=soft and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=hard and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=soft and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=hard and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=soft and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=hard and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=soft and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Test Accuracy with voting=hard and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 94.89%\n",
      "Test Accuracy with voting=soft and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 94.44%\n",
      "Failed with voting=hard and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=soft and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=hard and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=soft and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=hard and models (('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=soft and models (('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=hard and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Failed with voting=soft and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'auto', 'ovr', 'multinomial'}. Got 'deprecated' instead.\n",
      "Best Test Accuracy: 96.22% with voting=soft and models (('model_2', <catboost.core.CatBoostClassifier object at 0x00000239B4829C90>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)))\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "best_combination = None\n",
    "best_voting_scheme = None\n",
    "best_score = 0\n",
    "\n",
    "# Iterate over all possible combinations of models with at least 3 models\n",
    "for L in range(3, len(estimators) + 1):\n",
    "    for subset in itertools.combinations(estimators, L):\n",
    "        # Iterate over all voting schemes\n",
    "        for voting in voting_schemes:\n",
    "            try:\n",
    "                voting_clf = VotingClassifier(\n",
    "                    estimators=list(subset),\n",
    "                    voting=voting\n",
    "                )\n",
    "                voting_clf.fit(x_train, y_train)\n",
    "                y_pred_test = voting_clf.predict(x_test)\n",
    "                test_accuracy = accuracy_score(y_test, y_pred_test) * 100\n",
    "\n",
    "                if test_accuracy > best_score:\n",
    "                    best_score = test_accuracy\n",
    "                    best_combination = subset\n",
    "                    best_voting_scheme = voting\n",
    "\n",
    "                print(f\"Test Accuracy with voting={voting} and models {subset}: {test_accuracy:.2f}%\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed with voting={voting} and models {subset}: {e}\")\n",
    "\n",
    "print(f\"Best Test Accuracy: {best_score:.2f}% with voting={best_voting_scheme} and models {best_combination}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  96.22222222222221\n",
      "Recall:  96.22222222222221\n",
      "Precision:  96.21454451345755\n",
      "F1-Score:  96.2178100788094\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score: 0.9498176724866575\n",
      "Loaded model accuracy: 96.22222222222221\n",
      "VotingClassifier(estimators=[('model_2',\n",
      "                              <catboost.core.CatBoostClassifier object at 0x00000239B2973CD0>),\n",
      "                             ('model_3',\n",
      "                              Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                                              ('knn',\n",
      "                                               KNeighborsClassifier(metric='euclidean',\n",
      "                                                                    n_neighbors=3))])),\n",
      "                             ('model_6',\n",
      "                              RandomForestClassifier(bootstrap=False,\n",
      "                                                     max_depth=50,\n",
      "                                                     min_samples_leaf=2,\n",
      "                                                     min_samples_split=5,\n",
      "                                                     n_estimators=500,\n",
      "                                                     random_state=123))],\n",
      "                 voting='soft')\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Evaluate the best model\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=list(best_combination),\n",
    "    voting=best_voting_scheme\n",
    ")\n",
    "voting_clf.fit(x_train, y_train)\n",
    "y_pred_test = voting_clf.predict(x_test)\n",
    "\n",
    "print('Accuracy: ', accuracy_score(y_test, y_pred_test) * 100)\n",
    "print('Recall: ', recall_score(y_test, y_pred_test, average='weighted') * 100)\n",
    "print('Precision: ', precision_score(y_test, y_pred_test, average='weighted') * 100)\n",
    "print('F1-Score: ', f1_score(y_test, y_pred_test, average='weighted') * 100)\n",
    "\n",
    "if best_voting_scheme == 'soft':\n",
    "    # Compute ROC curve and AUC score\n",
    "    y_pred_prob = voting_clf.predict_proba(x_test)[:, 1]\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.0])\n",
    "    plt.title('ROC curve for classifier')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    print(\"AUC Score:\", roc_auc_score(y_test, y_pred_prob))\n",
    "\n",
    "# Save the best model\n",
    "joblib_file = \"best_voting_model.pkl\"\n",
    "joblib.dump(voting_clf, joblib_file)\n",
    "\n",
    "# Load and test the saved model\n",
    "loaded_model = joblib.load(joblib_file)\n",
    "print('Loaded model accuracy:', accuracy_score(y_test, loaded_model.predict(x_test)) * 100)\n",
    "print(loaded_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %%\n",
    "# Generate the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_test)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "cm_display = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=voting_clf.classes_)\n",
    "cm_display.plot(cmap='Blues', values_format='d')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import winsound\n",
    "\n",
    "# Play a sound to indicate the end of the process\n",
    "winsound.MessageBeep()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
