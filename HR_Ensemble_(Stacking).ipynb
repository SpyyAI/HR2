{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score, roc_curve, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "SEED = 123\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "random.seed(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Load dataset\n",
    "df = pd.read_csv('recruitment_data.csv')\n",
    "\n",
    "# Define the features and the target class\n",
    "x = df.drop(columns=['HiringDecision'], axis=1)\n",
    "y = df['HiringDecision']\n",
    "\n",
    "# Split the data\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=123)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Load the saved models\n",
    "model_paths = [\n",
    "    'best_svm_model.pkl',\n",
    "    'catBoost1model.pkl', \n",
    "    'best_knn_model.pkl',\n",
    "    'LR_model_imb.pkl',\n",
    "    'best_gnb_model.pkl',\n",
    "    'RFmodel.pkl',\n",
    "    'xgbimba.pkl',\n",
    "    'DTmodel89.pkl'\n",
    "]\n",
    "\n",
    "models = []\n",
    "for path in model_paths:\n",
    "    model = joblib.load(path)\n",
    "    if hasattr(model, 'fit'):\n",
    "        models.append(model)\n",
    "    else:\n",
    "        print(f\"Error: Loaded object from {path} is not a valid model.\")\n",
    "\n",
    "model_names = [f'model_{i+1}' for i in range(len(models))]\n",
    "estimators = list(zip(model_names, models))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Define possible final estimators\n",
    "final_estimators = [\n",
    "    LogisticRegression(),\n",
    "    RandomForestClassifier(),\n",
    "    GradientBoostingClassifier(),\n",
    "    SVC(probability=True),\n",
    "    KNeighborsClassifier(),\n",
    "    AdaBoostClassifier(),\n",
    "    XGBClassifier(),\n",
    "    GaussianNB(),\n",
    "    MLPClassifier()\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])),): 87.33%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])),): 80.67%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])),): 87.56%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])),): 87.33%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])),): 88.00%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])),): 87.33%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])),): 88.67%\n",
      "Test Accuracy with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])),): 87.33%\n",
      "Test Accuracy with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])),): 87.56%\n",
      "Test Accuracy with LogisticRegression() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>),): 97.11%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>),): 92.89%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>),): 96.22%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>),): 97.11%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>),): 97.11%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>),): 97.11%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>),): 97.11%\n",
      "Test Accuracy with GaussianNB() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>),): 96.67%\n",
      "Test Accuracy with MLPClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>),): 97.11%\n",
      "Test Accuracy with LogisticRegression() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])),): 83.33%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])),): 83.33%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])),): 83.33%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])),): 83.33%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])),): 71.33%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])),): 83.33%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])),): 83.33%\n",
      "Test Accuracy with GaussianNB() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])),): 83.33%\n",
      "Test Accuracy with MLPClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])),): 83.33%\n",
      "Failed with LogisticRegression() and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])),): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])),): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])),): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])),): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])),): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])),): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])),): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])),): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])),): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Test Accuracy with LogisticRegression() and models (('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])),): 88.44%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])),): 81.78%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])),): 88.00%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])),): 88.44%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])),): 87.78%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])),): 88.22%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])),): 87.56%\n",
      "Test Accuracy with GaussianNB() and models (('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])),): 88.00%\n",
      "Test Accuracy with MLPClassifier() and models (('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])),): 88.44%\n",
      "Test Accuracy with LogisticRegression() and models (('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)),): 95.33%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)),): 90.44%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)),): 93.78%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)),): 95.33%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)),): 94.89%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)),): 95.33%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)),): 96.22%\n",
      "Test Accuracy with GaussianNB() and models (('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)),): 95.33%\n",
      "Test Accuracy with MLPClassifier() and models (('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)),): 95.78%\n",
      "Test Accuracy with LogisticRegression() and models (('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)),): 92.89%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)),): 87.56%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)),): 93.78%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)),): 93.78%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)),): 92.89%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)),): 93.33%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)),): 93.56%\n",
      "Test Accuracy with GaussianNB() and models (('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)),): 93.33%\n",
      "Test Accuracy with MLPClassifier() and models (('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)),): 92.89%\n",
      "Test Accuracy with LogisticRegression() and models (('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))])),): 89.33%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))])),): 89.78%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))])),): 88.89%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))])),): 89.56%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))])),): 49.11%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))])),): 89.56%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))])),): 89.78%\n",
      "Test Accuracy with GaussianNB() and models (('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))])),): 89.56%\n",
      "Test Accuracy with MLPClassifier() and models (('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))])),): 89.33%\n",
      "Test Accuracy with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>)): 97.11%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>)): 96.89%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>)): 97.11%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>)): 97.11%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>)): 96.67%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>)): 96.89%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>)): 96.22%\n",
      "Test Accuracy with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>)): 96.67%\n",
      "Test Accuracy with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>)): 97.11%\n",
      "Test Accuracy with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))]))): 87.11%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))]))): 82.44%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))]))): 86.67%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))]))): 87.56%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))]))): 87.33%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))]))): 87.33%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))]))): 87.33%\n",
      "Test Accuracy with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))]))): 84.44%\n",
      "Test Accuracy with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))]))): 88.22%\n",
      "Failed with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Test Accuracy with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): 88.22%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): 87.33%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): 87.78%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): 88.22%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): 86.89%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): 88.44%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): 86.00%\n",
      "Test Accuracy with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): 86.44%\n",
      "Test Accuracy with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): 88.00%\n",
      "Test Accuracy with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 95.56%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 95.11%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 95.11%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 95.33%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 94.67%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 94.89%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 94.22%\n",
      "Test Accuracy with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 94.00%\n",
      "Test Accuracy with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 95.56%\n",
      "Test Accuracy with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 93.56%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 93.33%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 92.67%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 93.33%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 93.33%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 93.33%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 91.56%\n",
      "Test Accuracy with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 92.44%\n",
      "Test Accuracy with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 93.56%\n",
      "Test Accuracy with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 90.89%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 86.44%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 90.44%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 91.11%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 91.11%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 89.78%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 90.67%\n",
      "Test Accuracy with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 90.22%\n",
      "Test Accuracy with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 90.89%\n",
      "Test Accuracy with LogisticRegression() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))]))): 97.11%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))]))): 93.33%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))]))): 96.67%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))]))): 97.11%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))]))): 97.11%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))]))): 97.11%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))]))): 97.11%\n",
      "Test Accuracy with GaussianNB() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))]))): 96.67%\n",
      "Test Accuracy with MLPClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))]))): 97.11%\n",
      "Failed with LogisticRegression() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Test Accuracy with LogisticRegression() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): 97.11%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): 97.11%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): 96.00%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): 97.11%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): 96.89%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): 96.89%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): 97.11%\n",
      "Test Accuracy with GaussianNB() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): 96.67%\n",
      "Test Accuracy with MLPClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): 97.11%\n",
      "Test Accuracy with LogisticRegression() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 97.11%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 96.89%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 96.89%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 97.11%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 97.11%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 96.89%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 96.22%\n",
      "Test Accuracy with GaussianNB() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 96.67%\n",
      "Test Accuracy with MLPClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 96.89%\n",
      "Test Accuracy with LogisticRegression() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 96.89%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 96.67%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 96.22%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 97.11%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 96.89%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 96.67%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 95.78%\n",
      "Test Accuracy with GaussianNB() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 95.78%\n",
      "Test Accuracy with MLPClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 96.67%\n",
      "Test Accuracy with LogisticRegression() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 97.11%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 94.00%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.44%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 97.11%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.89%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 97.11%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 97.11%\n",
      "Test Accuracy with GaussianNB() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.00%\n",
      "Test Accuracy with MLPClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 97.11%\n",
      "Failed with LogisticRegression() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Test Accuracy with LogisticRegression() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): 86.67%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): 83.33%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): 88.22%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): 88.44%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): 86.89%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): 88.22%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): 87.78%\n",
      "Test Accuracy with GaussianNB() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): 84.22%\n",
      "Test Accuracy with MLPClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): 88.22%\n",
      "Test Accuracy with LogisticRegression() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 95.33%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 90.44%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 94.22%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 94.89%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 94.00%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 95.33%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 94.67%\n",
      "Test Accuracy with GaussianNB() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 89.11%\n",
      "Test Accuracy with MLPClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 94.89%\n",
      "Test Accuracy with LogisticRegression() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 92.89%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 88.67%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 94.22%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 93.78%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 92.89%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 93.33%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 93.56%\n",
      "Test Accuracy with GaussianNB() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 89.78%\n",
      "Test Accuracy with MLPClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 93.33%\n",
      "Test Accuracy with LogisticRegression() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 88.89%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 91.56%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 90.44%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 90.22%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 89.78%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 89.56%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 90.44%\n",
      "Test Accuracy with GaussianNB() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 85.78%\n",
      "Test Accuracy with MLPClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 88.89%\n",
      "Failed with LogisticRegression() and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with LogisticRegression() and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with LogisticRegression() and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with LogisticRegression() and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Test Accuracy with LogisticRegression() and models (('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 95.56%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 95.33%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 95.11%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 95.78%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 95.78%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 95.33%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 94.44%\n",
      "Test Accuracy with GaussianNB() and models (('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 93.78%\n",
      "Test Accuracy with MLPClassifier() and models (('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 95.56%\n",
      "Test Accuracy with LogisticRegression() and models (('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 93.11%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 92.67%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 93.56%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 93.78%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 92.22%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 93.33%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 91.11%\n",
      "Test Accuracy with GaussianNB() and models (('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 92.89%\n",
      "Test Accuracy with MLPClassifier() and models (('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 93.56%\n",
      "Test Accuracy with LogisticRegression() and models (('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 90.67%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 87.78%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 90.44%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 91.56%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 91.33%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 89.78%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 89.33%\n",
      "Test Accuracy with GaussianNB() and models (('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 89.78%\n",
      "Test Accuracy with MLPClassifier() and models (('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 91.11%\n",
      "Test Accuracy with LogisticRegression() and models (('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 94.44%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 94.22%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 95.56%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 95.33%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 95.11%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 95.56%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 92.67%\n",
      "Test Accuracy with GaussianNB() and models (('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 94.00%\n",
      "Test Accuracy with MLPClassifier() and models (('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 94.44%\n",
      "Test Accuracy with LogisticRegression() and models (('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 95.33%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 91.33%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 94.89%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 95.11%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 95.78%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 95.33%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 94.22%\n",
      "Test Accuracy with GaussianNB() and models (('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 94.00%\n",
      "Test Accuracy with MLPClassifier() and models (('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 95.33%\n",
      "Test Accuracy with LogisticRegression() and models (('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 95.11%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 90.67%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 93.56%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 95.11%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 94.44%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 93.33%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 93.11%\n",
      "Test Accuracy with GaussianNB() and models (('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 92.89%\n",
      "Test Accuracy with MLPClassifier() and models (('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 95.11%\n",
      "Test Accuracy with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))]))): 97.11%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))]))): 97.11%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))]))): 96.89%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))]))): 97.11%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))]))): 97.11%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))]))): 96.89%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))]))): 96.44%\n",
      "Test Accuracy with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))]))): 94.44%\n",
      "Test Accuracy with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))]))): 97.11%\n",
      "Failed with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Test Accuracy with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): 97.11%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): 97.11%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): 96.67%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): 97.11%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): 96.89%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): 96.89%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): 96.67%\n",
      "Test Accuracy with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): 95.56%\n",
      "Test Accuracy with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): 97.11%\n",
      "Test Accuracy with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 97.11%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 97.11%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 96.89%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 96.89%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 96.89%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 97.11%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 96.22%\n",
      "Test Accuracy with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 96.67%\n",
      "Test Accuracy with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 96.89%\n",
      "Test Accuracy with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 96.67%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 96.89%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 96.67%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 97.11%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 96.89%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 96.44%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 95.33%\n",
      "Test Accuracy with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 95.78%\n",
      "Test Accuracy with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 96.67%\n",
      "Test Accuracy with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 97.11%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 97.11%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.44%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 97.11%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 97.11%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 97.11%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.67%\n",
      "Test Accuracy with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 95.56%\n",
      "Test Accuracy with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 97.11%\n",
      "Failed with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Test Accuracy with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): 87.33%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): 87.56%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): 88.67%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): 88.67%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): 87.33%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): 88.00%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): 86.67%\n",
      "Test Accuracy with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): 85.11%\n",
      "Test Accuracy with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): 88.44%\n",
      "Test Accuracy with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 95.33%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 94.22%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 93.78%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 94.22%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 94.22%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 95.33%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 94.00%\n",
      "Test Accuracy with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 90.22%\n",
      "Test Accuracy with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 94.44%\n",
      "Test Accuracy with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 93.11%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 92.44%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 93.56%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 93.56%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 93.11%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 93.11%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 91.33%\n",
      "Test Accuracy with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 90.00%\n",
      "Test Accuracy with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 92.89%\n",
      "Test Accuracy with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 90.67%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 88.67%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 90.67%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 91.56%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 90.89%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 88.89%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 90.00%\n",
      "Test Accuracy with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 89.11%\n",
      "Test Accuracy with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 91.11%\n",
      "Failed with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Test Accuracy with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 95.56%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 94.89%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 94.44%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 95.33%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 95.78%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 95.56%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 94.89%\n",
      "Test Accuracy with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 92.67%\n",
      "Test Accuracy with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 95.78%\n",
      "Test Accuracy with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 93.56%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 94.22%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 93.78%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 93.78%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 94.00%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 93.56%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 92.22%\n",
      "Test Accuracy with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 92.00%\n",
      "Test Accuracy with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 93.56%\n",
      "Test Accuracy with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 90.89%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 90.44%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 90.44%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 91.56%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 90.44%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 90.67%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 91.33%\n",
      "Test Accuracy with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 90.22%\n",
      "Test Accuracy with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 91.33%\n",
      "Test Accuracy with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 94.89%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 95.33%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 94.89%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 94.89%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 94.44%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 95.33%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 92.89%\n",
      "Test Accuracy with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 92.89%\n",
      "Test Accuracy with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 94.67%\n",
      "Test Accuracy with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 94.89%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 94.67%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 94.67%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 95.56%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 94.44%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 95.33%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 93.78%\n",
      "Test Accuracy with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 93.11%\n",
      "Test Accuracy with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 95.11%\n",
      "Test Accuracy with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 94.44%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 93.11%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 94.67%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 94.22%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 93.56%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 94.00%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 92.22%\n",
      "Test Accuracy with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 93.11%\n",
      "Test Accuracy with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 94.22%\n",
      "Failed with LogisticRegression() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Test Accuracy with LogisticRegression() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): 97.11%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): 96.89%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): 96.00%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): 97.11%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): 97.11%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): 96.89%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): 96.89%\n",
      "Test Accuracy with GaussianNB() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): 94.67%\n",
      "Test Accuracy with MLPClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): 97.11%\n",
      "Test Accuracy with LogisticRegression() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 97.11%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 96.89%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 96.89%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 97.11%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 96.89%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 96.89%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 96.67%\n",
      "Test Accuracy with GaussianNB() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 96.44%\n",
      "Test Accuracy with MLPClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 97.11%\n",
      "Test Accuracy with LogisticRegression() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 96.89%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 96.89%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 96.22%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 96.67%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 96.67%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 96.89%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 95.78%\n",
      "Test Accuracy with GaussianNB() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 95.11%\n",
      "Test Accuracy with MLPClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 97.11%\n",
      "Test Accuracy with LogisticRegression() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 97.11%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 94.44%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.67%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 97.11%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.89%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 97.11%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 97.11%\n",
      "Test Accuracy with GaussianNB() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 94.44%\n",
      "Test Accuracy with MLPClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 97.11%\n",
      "Failed with LogisticRegression() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with LogisticRegression() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with LogisticRegression() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with LogisticRegression() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Test Accuracy with LogisticRegression() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 97.11%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 97.11%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 96.89%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 96.89%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 96.89%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 96.89%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 96.67%\n",
      "Test Accuracy with GaussianNB() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 96.67%\n",
      "Test Accuracy with MLPClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 97.11%\n",
      "Test Accuracy with LogisticRegression() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 96.67%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 96.89%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 96.44%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 96.89%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 96.89%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 96.67%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 96.67%\n",
      "Test Accuracy with GaussianNB() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 95.78%\n",
      "Test Accuracy with MLPClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 96.67%\n",
      "Test Accuracy with LogisticRegression() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.89%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 97.11%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.44%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 97.11%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 97.11%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.67%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 97.33%\n",
      "Test Accuracy with GaussianNB() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 95.78%\n",
      "Test Accuracy with MLPClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.89%\n",
      "Test Accuracy with LogisticRegression() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 96.67%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 96.89%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 97.11%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 96.89%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 96.89%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 96.67%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 95.56%\n",
      "Test Accuracy with GaussianNB() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 95.56%\n",
      "Test Accuracy with MLPClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 96.89%\n",
      "Test Accuracy with LogisticRegression() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 97.11%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 97.11%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.89%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 97.11%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.44%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.89%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 95.78%\n",
      "Test Accuracy with GaussianNB() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.22%\n",
      "Test Accuracy with MLPClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 97.11%\n",
      "Test Accuracy with LogisticRegression() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.89%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.67%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.44%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.89%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.89%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.89%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.00%\n",
      "Test Accuracy with GaussianNB() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 95.56%\n",
      "Test Accuracy with MLPClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.89%\n",
      "Failed with LogisticRegression() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with LogisticRegression() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with LogisticRegression() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with LogisticRegression() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Test Accuracy with LogisticRegression() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 95.33%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 94.67%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 94.44%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 93.78%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 93.33%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 94.44%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 94.00%\n",
      "Test Accuracy with GaussianNB() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 90.00%\n",
      "Test Accuracy with MLPClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 94.67%\n",
      "Test Accuracy with LogisticRegression() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 93.11%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 93.33%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 94.00%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 94.00%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 93.56%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 93.33%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 91.33%\n",
      "Test Accuracy with GaussianNB() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 89.33%\n",
      "Test Accuracy with MLPClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 92.89%\n",
      "Test Accuracy with LogisticRegression() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 90.89%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 88.00%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 91.11%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 91.33%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 90.44%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 89.78%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 89.78%\n",
      "Test Accuracy with GaussianNB() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 88.44%\n",
      "Test Accuracy with MLPClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 90.89%\n",
      "Test Accuracy with LogisticRegression() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 94.89%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 93.78%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 95.11%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 94.44%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 93.78%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 95.56%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 93.11%\n",
      "Test Accuracy with GaussianNB() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 92.44%\n",
      "Test Accuracy with MLPClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 94.89%\n",
      "Test Accuracy with LogisticRegression() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 95.33%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 93.56%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 94.00%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 95.11%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 94.00%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 95.78%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 94.22%\n",
      "Test Accuracy with GaussianNB() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 90.22%\n",
      "Test Accuracy with MLPClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.00%\n",
      "Test Accuracy with LogisticRegression() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 95.11%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 91.56%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 94.44%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 95.11%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 94.00%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 93.33%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 94.22%\n",
      "Test Accuracy with GaussianNB() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 91.33%\n",
      "Test Accuracy with MLPClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 94.89%\n",
      "Failed with LogisticRegression() and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with LogisticRegression() and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with LogisticRegression() and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with LogisticRegression() and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with LogisticRegression() and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with LogisticRegression() and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Test Accuracy with LogisticRegression() and models (('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 94.89%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 95.33%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 95.33%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 95.11%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 94.00%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 95.33%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 93.11%\n",
      "Test Accuracy with GaussianNB() and models (('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 93.33%\n",
      "Test Accuracy with MLPClassifier() and models (('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 94.89%\n",
      "Test Accuracy with LogisticRegression() and models (('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 95.11%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 95.33%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 95.33%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.00%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 95.56%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 94.89%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 93.78%\n",
      "Test Accuracy with GaussianNB() and models (('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 93.56%\n",
      "Test Accuracy with MLPClassifier() and models (('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 95.11%\n",
      "Test Accuracy with LogisticRegression() and models (('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 94.67%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 93.11%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 93.78%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 94.00%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 93.33%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 93.33%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 92.44%\n",
      "Test Accuracy with GaussianNB() and models (('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 93.56%\n",
      "Test Accuracy with MLPClassifier() and models (('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 94.67%\n",
      "Test Accuracy with LogisticRegression() and models (('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 95.33%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 94.67%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 95.11%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 95.33%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 95.11%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 95.11%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 94.00%\n",
      "Test Accuracy with GaussianNB() and models (('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 93.56%\n",
      "Test Accuracy with MLPClassifier() and models (('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 95.11%\n",
      "Failed with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Test Accuracy with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): 97.11%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): 97.11%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): 97.11%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): 97.11%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): 97.11%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): 96.00%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): 96.89%\n",
      "Test Accuracy with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): 93.33%\n",
      "Test Accuracy with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): 97.11%\n",
      "Test Accuracy with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 97.11%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 97.11%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 96.22%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 96.89%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 96.89%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 96.22%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 96.89%\n",
      "Test Accuracy with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 95.33%\n",
      "Test Accuracy with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 96.89%\n",
      "Test Accuracy with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 96.67%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 96.89%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 96.89%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 96.67%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 96.67%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 96.67%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 94.44%\n",
      "Test Accuracy with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 94.67%\n",
      "Test Accuracy with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 97.11%\n",
      "Test Accuracy with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 97.11%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 97.11%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.89%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 97.11%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 97.11%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.67%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.44%\n",
      "Test Accuracy with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 93.56%\n",
      "Test Accuracy with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 97.11%\n",
      "Failed with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Test Accuracy with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 97.11%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 97.11%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 96.89%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 96.89%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 96.89%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 96.67%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 96.89%\n",
      "Test Accuracy with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 95.78%\n",
      "Test Accuracy with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 96.89%\n",
      "Test Accuracy with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 96.67%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 97.11%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 97.11%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 96.89%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 96.89%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 96.22%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 96.00%\n",
      "Test Accuracy with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 94.89%\n",
      "Test Accuracy with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 96.67%\n",
      "Test Accuracy with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.89%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 97.11%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.89%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 97.11%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 97.11%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.67%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.89%\n",
      "Test Accuracy with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 95.33%\n",
      "Test Accuracy with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 97.11%\n",
      "Test Accuracy with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 96.67%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 97.11%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 97.11%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 96.89%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 96.89%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 96.67%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 96.22%\n",
      "Test Accuracy with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 95.56%\n",
      "Test Accuracy with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 97.11%\n",
      "Test Accuracy with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.89%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 97.11%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.89%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.89%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.89%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.67%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.89%\n",
      "Test Accuracy with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 95.33%\n",
      "Test Accuracy with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 97.11%\n",
      "Test Accuracy with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.89%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 97.11%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.67%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 97.11%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.89%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.44%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 95.78%\n",
      "Test Accuracy with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 95.11%\n",
      "Test Accuracy with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.89%\n",
      "Failed with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Test Accuracy with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 95.33%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 94.67%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 94.67%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 94.00%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 94.67%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 94.89%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 94.22%\n",
      "Test Accuracy with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 90.00%\n",
      "Test Accuracy with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 94.89%\n",
      "Test Accuracy with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 93.11%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 94.22%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 93.33%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 93.78%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 93.11%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 93.56%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 92.67%\n",
      "Test Accuracy with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 90.00%\n",
      "Test Accuracy with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 93.78%\n",
      "Test Accuracy with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 90.67%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 91.78%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 90.44%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 91.78%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 91.33%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 90.44%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 90.44%\n",
      "Test Accuracy with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 89.56%\n",
      "Test Accuracy with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 91.11%\n",
      "Test Accuracy with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 94.67%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 95.11%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 94.89%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 94.00%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 94.22%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 95.33%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 93.56%\n",
      "Test Accuracy with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 91.56%\n",
      "Test Accuracy with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 94.67%\n",
      "Test Accuracy with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 95.33%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 95.33%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 94.44%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 95.11%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 94.89%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 95.33%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 93.78%\n",
      "Test Accuracy with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 91.11%\n",
      "Test Accuracy with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 95.33%\n",
      "Test Accuracy with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 94.44%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 94.22%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 94.44%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 94.67%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 94.44%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 93.33%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 93.33%\n",
      "Test Accuracy with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 91.56%\n",
      "Test Accuracy with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 94.22%\n",
      "Failed with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Test Accuracy with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 94.89%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 95.56%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 95.33%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 94.89%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 95.11%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 95.78%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 94.22%\n",
      "Test Accuracy with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 92.89%\n",
      "Test Accuracy with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 94.67%\n",
      "Test Accuracy with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 94.89%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 95.33%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 94.89%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 95.78%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 94.44%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 95.11%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 94.00%\n",
      "Test Accuracy with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 92.89%\n",
      "Test Accuracy with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 95.56%\n",
      "Test Accuracy with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 94.44%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 94.00%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 94.00%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 93.78%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 93.78%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 93.33%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 93.78%\n",
      "Test Accuracy with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 92.67%\n",
      "Test Accuracy with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 94.44%\n",
      "Test Accuracy with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 94.67%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 94.89%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 95.33%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 94.89%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 94.22%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 95.33%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 93.78%\n",
      "Test Accuracy with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 93.56%\n",
      "Test Accuracy with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 94.67%\n",
      "Failed with LogisticRegression() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with LogisticRegression() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with LogisticRegression() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with LogisticRegression() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Test Accuracy with LogisticRegression() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 97.11%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 97.11%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 96.67%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 96.89%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 96.89%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 96.44%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 96.89%\n",
      "Test Accuracy with GaussianNB() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 95.56%\n",
      "Test Accuracy with MLPClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 96.89%\n",
      "Test Accuracy with LogisticRegression() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 96.67%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 96.89%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 96.44%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 96.67%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 96.89%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 96.67%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 96.67%\n",
      "Test Accuracy with GaussianNB() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 95.33%\n",
      "Test Accuracy with MLPClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 97.11%\n",
      "Test Accuracy with LogisticRegression() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 97.11%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.89%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.44%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 97.11%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 97.11%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.89%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 97.11%\n",
      "Test Accuracy with GaussianNB() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 94.00%\n",
      "Test Accuracy with MLPClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 97.11%\n",
      "Test Accuracy with LogisticRegression() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 96.89%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 96.89%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 97.11%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 96.67%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 96.67%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 96.44%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 95.33%\n",
      "Test Accuracy with GaussianNB() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 95.33%\n",
      "Test Accuracy with MLPClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 97.11%\n",
      "Test Accuracy with LogisticRegression() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 97.11%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.89%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.67%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 97.11%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.89%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.89%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.67%\n",
      "Test Accuracy with GaussianNB() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 95.78%\n",
      "Test Accuracy with MLPClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 97.11%\n",
      "Test Accuracy with LogisticRegression() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.89%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.44%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.44%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.67%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.89%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.89%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.44%\n",
      "Test Accuracy with GaussianNB() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 95.33%\n",
      "Test Accuracy with MLPClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 97.11%\n",
      "Failed with LogisticRegression() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with LogisticRegression() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with LogisticRegression() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with LogisticRegression() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with LogisticRegression() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with LogisticRegression() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Test Accuracy with LogisticRegression() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 96.67%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 97.11%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 96.67%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 96.89%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 96.89%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 96.67%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 96.00%\n",
      "Test Accuracy with GaussianNB() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 95.56%\n",
      "Test Accuracy with MLPClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 96.67%\n",
      "Test Accuracy with LogisticRegression() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.89%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 97.11%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.67%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 97.11%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 97.11%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.89%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.89%\n",
      "Test Accuracy with GaussianNB() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.22%\n",
      "Test Accuracy with MLPClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.89%\n",
      "Test Accuracy with LogisticRegression() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.89%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 97.11%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.89%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 97.11%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 97.11%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.67%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.22%\n",
      "Test Accuracy with GaussianNB() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 95.11%\n",
      "Test Accuracy with MLPClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.89%\n",
      "Test Accuracy with LogisticRegression() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.89%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 97.11%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.89%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 97.11%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.89%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.67%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.89%\n",
      "Test Accuracy with GaussianNB() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 95.56%\n",
      "Test Accuracy with MLPClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 97.11%\n",
      "Failed with LogisticRegression() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with LogisticRegression() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with LogisticRegression() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with LogisticRegression() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with LogisticRegression() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with LogisticRegression() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Test Accuracy with LogisticRegression() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 94.67%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 94.89%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 95.11%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 94.22%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 94.00%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 95.78%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 93.33%\n",
      "Test Accuracy with GaussianNB() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 91.78%\n",
      "Test Accuracy with MLPClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 94.89%\n",
      "Test Accuracy with LogisticRegression() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 95.78%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 95.56%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 95.33%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 95.11%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 94.44%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 95.56%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 93.33%\n",
      "Test Accuracy with GaussianNB() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 91.11%\n",
      "Test Accuracy with MLPClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 95.56%\n",
      "Test Accuracy with LogisticRegression() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 94.67%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 92.89%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 93.56%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 94.00%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 94.44%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 92.89%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 92.67%\n",
      "Test Accuracy with GaussianNB() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 91.33%\n",
      "Test Accuracy with MLPClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 94.44%\n",
      "Test Accuracy with LogisticRegression() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 95.78%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 95.11%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 95.11%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 95.78%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 94.22%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 95.78%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 92.89%\n",
      "Test Accuracy with GaussianNB() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 92.44%\n",
      "Test Accuracy with MLPClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.00%\n",
      "Failed with LogisticRegression() and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with LogisticRegression() and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with LogisticRegression() and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with LogisticRegression() and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Test Accuracy with LogisticRegression() and models (('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 94.89%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 95.56%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 95.11%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 95.11%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 94.89%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 95.11%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 94.22%\n",
      "Test Accuracy with GaussianNB() and models (('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 93.78%\n",
      "Test Accuracy with MLPClassifier() and models (('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 95.11%\n",
      "Failed with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Test Accuracy with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 97.11%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 97.11%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 96.67%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 96.89%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 96.89%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 97.11%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 97.11%\n",
      "Test Accuracy with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 94.00%\n",
      "Test Accuracy with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): 96.89%\n",
      "Test Accuracy with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 96.67%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 97.11%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 96.89%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 96.67%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 96.89%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 96.67%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 96.00%\n",
      "Test Accuracy with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 94.44%\n",
      "Test Accuracy with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 96.89%\n",
      "Test Accuracy with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 97.11%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 97.11%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.67%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 97.11%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 97.11%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.22%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 97.11%\n",
      "Test Accuracy with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 93.33%\n",
      "Test Accuracy with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 97.11%\n",
      "Test Accuracy with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 96.89%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 97.11%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 97.11%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 96.67%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 96.67%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 96.67%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 96.22%\n",
      "Test Accuracy with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 94.67%\n",
      "Test Accuracy with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 97.11%\n",
      "Test Accuracy with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 97.11%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 97.11%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.89%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 97.11%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 97.11%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.67%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.22%\n",
      "Test Accuracy with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 94.22%\n",
      "Test Accuracy with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 97.11%\n",
      "Test Accuracy with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.89%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 97.11%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.89%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.89%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.89%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.67%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.00%\n",
      "Test Accuracy with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 94.00%\n",
      "Test Accuracy with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 97.11%\n",
      "Failed with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Test Accuracy with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 96.67%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 97.11%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 97.11%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 96.89%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 96.89%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 96.67%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 96.67%\n",
      "Test Accuracy with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 95.11%\n",
      "Test Accuracy with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 96.89%\n",
      "Test Accuracy with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.89%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 97.11%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.67%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.89%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.89%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.89%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 97.11%\n",
      "Test Accuracy with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 94.89%\n",
      "Test Accuracy with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 97.11%\n",
      "Test Accuracy with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.89%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 97.11%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 97.11%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 97.11%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 97.11%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.22%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.00%\n",
      "Test Accuracy with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 94.67%\n",
      "Test Accuracy with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.89%\n",
      "Test Accuracy with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.89%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 97.11%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.89%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 97.11%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.89%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.67%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.67%\n",
      "Test Accuracy with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 95.33%\n",
      "Test Accuracy with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 97.11%\n",
      "Failed with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Test Accuracy with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 94.67%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 94.89%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 94.89%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 93.78%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 93.11%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 95.33%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 94.00%\n",
      "Test Accuracy with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 91.11%\n",
      "Test Accuracy with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 94.44%\n",
      "Test Accuracy with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 95.33%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.44%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 94.44%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 94.89%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 94.67%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 95.33%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 93.56%\n",
      "Test Accuracy with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 91.11%\n",
      "Test Accuracy with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 95.56%\n",
      "Test Accuracy with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 94.44%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 93.56%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 93.78%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 94.22%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 94.44%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 93.33%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 91.78%\n",
      "Test Accuracy with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 90.89%\n",
      "Test Accuracy with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 94.22%\n",
      "Test Accuracy with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 95.33%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 95.33%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 95.11%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 95.11%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 94.89%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 95.56%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 93.11%\n",
      "Test Accuracy with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 92.00%\n",
      "Test Accuracy with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 95.78%\n",
      "Failed with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Test Accuracy with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 94.67%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 95.33%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 95.56%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 94.89%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 94.00%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 95.33%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 93.33%\n",
      "Test Accuracy with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 93.78%\n",
      "Test Accuracy with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 94.67%\n",
      "Failed with LogisticRegression() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with LogisticRegression() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with LogisticRegression() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with LogisticRegression() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with LogisticRegression() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with LogisticRegression() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Test Accuracy with LogisticRegression() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 96.89%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 97.11%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 96.89%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 96.67%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 96.89%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 96.67%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 95.78%\n",
      "Test Accuracy with GaussianNB() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 95.11%\n",
      "Test Accuracy with MLPClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 96.89%\n",
      "Test Accuracy with LogisticRegression() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 97.11%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 97.11%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.89%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 97.11%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 97.11%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.67%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.89%\n",
      "Test Accuracy with GaussianNB() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 95.56%\n",
      "Test Accuracy with MLPClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 97.11%\n",
      "Test Accuracy with LogisticRegression() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.89%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.89%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.89%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.89%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.89%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.67%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.44%\n",
      "Test Accuracy with GaussianNB() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 94.89%\n",
      "Test Accuracy with MLPClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.89%\n",
      "Test Accuracy with LogisticRegression() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.89%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 97.11%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 97.11%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 97.11%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.89%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.67%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.00%\n",
      "Test Accuracy with GaussianNB() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 95.33%\n",
      "Test Accuracy with MLPClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 97.11%\n",
      "Failed with LogisticRegression() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with LogisticRegression() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with LogisticRegression() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with LogisticRegression() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Test Accuracy with LogisticRegression() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.89%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 97.11%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.89%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 97.11%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 97.11%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.67%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 97.11%\n",
      "Test Accuracy with GaussianNB() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 95.56%\n",
      "Test Accuracy with MLPClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.89%\n",
      "Failed with LogisticRegression() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with LogisticRegression() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with LogisticRegression() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with LogisticRegression() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Test Accuracy with LogisticRegression() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 95.33%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 95.56%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 94.89%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 94.89%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 95.11%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 95.11%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 93.56%\n",
      "Test Accuracy with GaussianNB() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 91.78%\n",
      "Test Accuracy with MLPClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.00%\n",
      "Failed with LogisticRegression() and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Test Accuracy with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 96.89%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 97.11%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 97.11%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 96.67%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 96.89%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 96.89%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 96.22%\n",
      "Test Accuracy with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 94.44%\n",
      "Test Accuracy with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): 97.11%\n",
      "Test Accuracy with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 97.11%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 97.11%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.67%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 97.11%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 97.11%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.22%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 97.11%\n",
      "Test Accuracy with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 94.00%\n",
      "Test Accuracy with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 97.11%\n",
      "Test Accuracy with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.89%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 97.11%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 97.11%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.89%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.89%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.44%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.00%\n",
      "Test Accuracy with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 93.11%\n",
      "Test Accuracy with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 97.11%\n",
      "Test Accuracy with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.89%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 97.11%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.89%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.89%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.67%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.89%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.44%\n",
      "Test Accuracy with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 94.67%\n",
      "Test Accuracy with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 97.11%\n",
      "Failed with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Test Accuracy with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.89%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 97.11%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.89%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.89%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.67%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.00%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.89%\n",
      "Test Accuracy with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 94.44%\n",
      "Test Accuracy with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.89%\n",
      "Failed with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Test Accuracy with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 95.11%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 95.33%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 95.11%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 95.11%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 95.33%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 95.11%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 92.89%\n",
      "Test Accuracy with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 91.78%\n",
      "Test Accuracy with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 95.56%\n",
      "Failed with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with LogisticRegression() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with LogisticRegression() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with LogisticRegression() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with LogisticRegression() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Test Accuracy with LogisticRegression() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.89%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.89%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.89%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.89%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.89%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.67%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 97.11%\n",
      "Test Accuracy with GaussianNB() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 94.89%\n",
      "Test Accuracy with MLPClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 97.11%\n",
      "Failed with LogisticRegression() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with LogisticRegression() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Test Accuracy with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.67%\n",
      "Test Accuracy with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 97.11%\n",
      "Test Accuracy with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.67%\n",
      "Test Accuracy with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.67%\n",
      "Test Accuracy with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.67%\n",
      "Test Accuracy with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.67%\n",
      "Test Accuracy with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 96.89%\n",
      "Test Accuracy with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 94.22%\n",
      "Test Accuracy with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): 97.11%\n",
      "Failed with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with LogisticRegression() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with LogisticRegression() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with RandomForestClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GradientBoostingClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with SVC(probability=True) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with KNeighborsClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with AdaBoostClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with GaussianNB() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Failed with MLPClassifier() and models (('model_1', Pipeline(steps=[('selectkbest', SelectKBest(k=7)),\n",
      "                ('svc',\n",
      "                 SVC(C=0.1, gamma=1, kernel='linear', probability=True))])), ('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_3', Pipeline(steps=[('selectkbest', SelectKBest(k=1)),\n",
      "                ('knn',\n",
      "                 KNeighborsClassifier(metric='euclidean', n_neighbors=3))])), ('model_4', Pipeline(steps=[('feature_selection', SelectKBest(k=8)),\n",
      "                ('logistic_regression',\n",
      "                 LogisticRegression(C=0.1, multi_class='deprecated',\n",
      "                                    solver='liblinear'))])), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_6', RandomForestClassifier(bootstrap=False, max_depth=50, min_samples_leaf=2,\n",
      "                       min_samples_split=5, n_estimators=500, random_state=123)), ('model_7', XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depg=7, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=500, n_jobs=None,\n",
      "              num_parallel_tree=None, ...)), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))]))): The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'deprecated' instead.\n",
      "Best Test Accuracy: 97.33% with XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...) and models (('model_2', <catboost.core.CatBoostClassifier object at 0x0000012D6E0F7750>), ('model_5', Pipeline(steps=[('selectkbest', SelectKBest(k=6)),\n",
      "                ('gnb', GaussianNB(var_smoothing=5.336699231206313e-06))])), ('model_8', Pipeline(steps=[('smote', SMOTE(random_state=123)),\n",
      "                ('select_k_best', SelectKBest(k=8)),\n",
      "                ('classifier',\n",
      "                 DecisionTreeClassifier(criterion='entropy', max_depth=8,\n",
      "                                        min_samples_leaf=4,\n",
      "                                        min_samples_split=10,\n",
      "                                        random_state=123))])))\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "best_combination = None\n",
    "best_final_estimator = None\n",
    "best_score = 0\n",
    "\n",
    "# Iterate over all possible combinations of models\n",
    "for L in range(1, len(estimators) + 1):\n",
    "    for subset in itertools.combinations(estimators, L):\n",
    "        # Iterate over all final estimators\n",
    "        for final_estimator in final_estimators:\n",
    "            try:\n",
    "                stacking_clf = StackingClassifier(\n",
    "                    estimators=list(subset),\n",
    "                    final_estimator=final_estimator,\n",
    "                    cv=5\n",
    "                )\n",
    "                stacking_clf.fit(x_train, y_train)\n",
    "                y_pred_test = stacking_clf.predict(x_test)\n",
    "                test_accuracy = accuracy_score(y_test, y_pred_test) * 100\n",
    "\n",
    "                if test_accuracy > best_score:\n",
    "                    best_score = test_accuracy\n",
    "                    best_combination = subset\n",
    "                    best_final_estimator = final_estimator\n",
    "\n",
    "                print(f\"Test Accuracy with {final_estimator} and models {subset}: {test_accuracy:.2f}%\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed with {final_estimator} and models {subset}: {e}\")\n",
    "\n",
    "print(f\"Best Test Accuracy: {best_score:.2f}% with {best_final_estimator} and models {best_combination}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  97.33333333333334\n",
      "Recall:  97.33333333333334\n",
      "Precision:  97.32676923076924\n",
      "F1-Score:  97.32059189195778\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score: 0.9613972807843706\n",
      "Loaded model accuracy: 97.33333333333334\n",
      "StackingClassifier(cv=5,\n",
      "                   estimators=[('model_2',\n",
      "                                <catboost.core.CatBoostClassifier object at 0x0000012D6D94D590>),\n",
      "                               ('model_5',\n",
      "                                Pipeline(steps=[('selectkbest',\n",
      "                                                 SelectKBest(k=6)),\n",
      "                                                ('gnb',\n",
      "                                                 GaussianNB(var_smoothing=5.336699231206313e-06))])),\n",
      "                               ('model_8',\n",
      "                                Pipeline(steps=[('smote',\n",
      "                                                 SMOTE(random_state=123)),\n",
      "                                                ('select_k_best',\n",
      "                                                 SelectKBest(k=8)),\n",
      "                                                ('classifier',\n",
      "                                                 DecisionTree...\n",
      "                                                 grow_policy=None,\n",
      "                                                 importance_type=None,\n",
      "                                                 interaction_constraints=None,\n",
      "                                                 learning_rate=None,\n",
      "                                                 max_bin=None,\n",
      "                                                 max_cat_threshold=None,\n",
      "                                                 max_cat_to_onehot=None,\n",
      "                                                 max_delta_step=None,\n",
      "                                                 max_depth=None,\n",
      "                                                 max_leaves=None,\n",
      "                                                 min_child_weight=None,\n",
      "                                                 missing=nan,\n",
      "                                                 monotone_constraints=None,\n",
      "                                                 multi_strategy=None,\n",
      "                                                 n_estimators=None, n_jobs=None,\n",
      "                                                 num_parallel_tree=None,\n",
      "                                                 random_state=None, ...))\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Evaluate the best model\n",
    "stacking_clf = StackingClassifier(\n",
    "    estimators=list(best_combination),\n",
    "    final_estimator=best_final_estimator,\n",
    "    cv=5\n",
    ")\n",
    "stacking_clf.fit(x_train, y_train)\n",
    "y_pred_test = stacking_clf.predict(x_test)\n",
    "\n",
    "print('Accuracy: ', accuracy_score(y_test, y_pred_test) * 100)\n",
    "print('Recall: ', recall_score(y_test, y_pred_test, average='weighted') * 100)\n",
    "print('Precision: ', precision_score(y_test, y_pred_test, average='weighted') * 100)\n",
    "print('F1-Score: ', f1_score(y_test, y_pred_test, average='weighted') * 100)\n",
    "\n",
    "# Compute ROC curve and AUC score\n",
    "y_pred_prob = stacking_clf.predict_proba(x_test)[:, 1]\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.title('ROC curve for classifier')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(\"AUC Score:\", roc_auc_score(y_test, y_pred_prob))\n",
    "\n",
    "# Save the best model\n",
    "joblib_file = \"best_stacking_model.pkl\"\n",
    "joblib.dump(stacking_clf, joblib_file)\n",
    "\n",
    "# Load and test the saved model\n",
    "loaded_model = joblib.load(joblib_file)\n",
    "print('Loaded model accuracy:', accuracy_score(y_test, loaded_model.predict(x_test)) * 100)\n",
    "print(loaded_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %%\n",
    "# Generate the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_test)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "cm_display = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=stacking_clf.classes_)\n",
    "cm_display.plot(cmap='Blues', values_format='d')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
